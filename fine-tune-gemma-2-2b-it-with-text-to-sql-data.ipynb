{"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01d6ddac108a46f0b34a8ebc6ecb4a33":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09308312030d4d75a2c13ac59becafa9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b46156d7bef45dc98b414cfa59c58f5","max":819,"min":0,"orientation":"horizontal","style":"IPY_MODEL_83d7c0439c504879a2e27d24b6972038","value":819}},"09391d8b737a47ed859a7d985c108272":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b46156d7bef45dc98b414cfa59c58f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f7df3565b9c490fbf160dee66f072bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaf431dcd5f74b15ac953a1cc78f9eba","placeholder":"​","style":"IPY_MODEL_1bef78565e514c93a63f052972dff5c8","value":"Saving the dataset (1/1 shards): 100%"}},"1497de94cc784e6aaae149c257f12615":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26c55b8aa6d44de7b2d3f61fc7c1d92d","placeholder":"​","style":"IPY_MODEL_79fda5de6f6543f5aa35e5c7e2cd4513","value":" 16/16 [00:03&lt;00:00,  4.47ba/s]"}},"149fed4dd0af42cbac6ecf91aae96a95":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bef78565e514c93a63f052972dff5c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22788242a4154607be60109abc2e1c88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22c63ed3758f47ef97a6606913493e16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_be5d717b5eeb438b902c23454a5add5e","max":14732,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09391d8b737a47ed859a7d985c108272","value":14732}},"258b6cf54bb24a4593da93c79d5e535e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26c55b8aa6d44de7b2d3f61fc7c1d92d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2994e876d1b94608b1ddd95c10f105d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"2d5c032c012942b5af8f54a0cda4bc57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45f9d0d5af264eb7849c4d0f54ad9bda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0f7df3565b9c490fbf160dee66f072bf","IPY_MODEL_22c63ed3758f47ef97a6606913493e16","IPY_MODEL_e194a109192e4451b27fa84c4c62d035"],"layout":"IPY_MODEL_e50462043a494cb4adbee14a2d22a98a"}},"48f41cd576454f538d83ab27b50034de":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53f8a742f41a45c69b999bdc475ce430":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aa70a2ee95b64d129ce2eeb5a247bb24","IPY_MODEL_b108f9f3d55c4983aa342465ddd2bf96","IPY_MODEL_e5bb6b1babce463687f5e75ef56c468f"],"layout":"IPY_MODEL_01d6ddac108a46f0b34a8ebc6ecb4a33"}},"578f354fa89f4f3e9a93fd8ee248c9ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1201bf45509432693eb958d61f2e9cf","placeholder":"​","style":"IPY_MODEL_e23c50a9f877446f8a185d998929b07c","value":"100%"}},"6e57ad2848fe4f2bb0f89c79e4a41f54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a452b361c9147fdb46bd0cdd3ab8ec2","placeholder":"​","style":"IPY_MODEL_cd993d7aaabb417e9523edcb036ae58c","value":" 1/1 [00:01&lt;00:00,  1.00s/ba]"}},"78f28b924827405c88e26fb3bb7babab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79fda5de6f6543f5aa35e5c7e2cd4513":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a452b361c9147fdb46bd0cdd3ab8ec2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83d7c0439c504879a2e27d24b6972038":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"866d10c8593a48b191625f8e88ea37bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c38113fe88314af5b73495d2869b0baf","IPY_MODEL_09308312030d4d75a2c13ac59becafa9","IPY_MODEL_ea88c4ce38584d2982fe4976b3b2e9dc"],"layout":"IPY_MODEL_2994e876d1b94608b1ddd95c10f105d3"}},"8f91724f405942a59d0ad87a887095da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ff7f533460c480d8828bfdcfe313d7b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93b8752310254f9d97ef252e1fdd7c22":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_578f354fa89f4f3e9a93fd8ee248c9ee","IPY_MODEL_b9d0cd91e7b248358919101528b22ec7","IPY_MODEL_6e57ad2848fe4f2bb0f89c79e4a41f54"],"layout":"IPY_MODEL_149fed4dd0af42cbac6ecf91aae96a95"}},"95272f282a1742618ec5fecae87ba97e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a69f1452bb34a6495ebfe376c7930ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e8c25dffdfa431b8df9ee5f9fffafa6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f6bac398f734e1c9beb259289baf49b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa70a2ee95b64d129ce2eeb5a247bb24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_258b6cf54bb24a4593da93c79d5e535e","placeholder":"​","style":"IPY_MODEL_e8f8c2a564094f6c9e984d1de7b4a536","value":"100%"}},"aaf431dcd5f74b15ac953a1cc78f9eba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b108f9f3d55c4983aa342465ddd2bf96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f6bac398f734e1c9beb259289baf49b","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dbf8d79e1bcf41c7aacf805475f593a3","value":3}},"b5ef42051d9e46fb9537972e2c645663":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8736b2ef5fc4bbba9a1946d0d1951c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48f41cd576454f538d83ab27b50034de","placeholder":"​","style":"IPY_MODEL_9e8c25dffdfa431b8df9ee5f9fffafa6","value":"100%"}},"b9d0cd91e7b248358919101528b22ec7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5f944d2f84841b1b8e4dcdeb8ab7e0d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c4d2cbe030564618b312315e116bc3dd","value":1}},"bb25b0588d4149a2b9c9dbeb327b2a33":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be5d717b5eeb438b902c23454a5add5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c122efad9a644f6a8a4d686f9707ece7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b8736b2ef5fc4bbba9a1946d0d1951c3","IPY_MODEL_fbf0168e87974d47a35a13bf2b66a72d","IPY_MODEL_1497de94cc784e6aaae149c257f12615"],"layout":"IPY_MODEL_b5ef42051d9e46fb9537972e2c645663"}},"c38113fe88314af5b73495d2869b0baf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ff7f533460c480d8828bfdcfe313d7b","placeholder":"​","style":"IPY_MODEL_78f28b924827405c88e26fb3bb7babab","value":"Saving the dataset (1/1 shards): 100%"}},"c4d2cbe030564618b312315e116bc3dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c5f944d2f84841b1b8e4dcdeb8ab7e0d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd993d7aaabb417e9523edcb036ae58c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbf8d79e1bcf41c7aacf805475f593a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e194a109192e4451b27fa84c4c62d035":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95272f282a1742618ec5fecae87ba97e","placeholder":"​","style":"IPY_MODEL_2d5c032c012942b5af8f54a0cda4bc57","value":" 14732/14732 [00:00&lt;00:00, 114690.83 examples/s]"}},"e23c50a9f877446f8a185d998929b07c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e50462043a494cb4adbee14a2d22a98a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"e5bb6b1babce463687f5e75ef56c468f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22788242a4154607be60109abc2e1c88","placeholder":"​","style":"IPY_MODEL_9a69f1452bb34a6495ebfe376c7930ea","value":" 3/3 [00:00&lt;00:00,  9.80it/s]"}},"e8f8c2a564094f6c9e984d1de7b4a536":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea88c4ce38584d2982fe4976b3b2e9dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb25b0588d4149a2b9c9dbeb327b2a33","placeholder":"​","style":"IPY_MODEL_8f91724f405942a59d0ad87a887095da","value":" 819/819 [00:00&lt;00:00, 23586.16 examples/s]"}},"f070e098fab84c50b7223d6424a9d369":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f1201bf45509432693eb958d61f2e9cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbf0168e87974d47a35a13bf2b66a72d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc8a9c5ca08e491d883ce8006d200004","max":16,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f070e098fab84c50b7223d6424a9d369","value":16}},"fc8a9c5ca08e491d883ce8006d200004":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9529712,"sourceType":"datasetVersion","datasetId":5803429}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Fine-tune Gemma-2-2b-it With Text-to-SQL Data**\n\n**Test Result Version 1:**\n### Evaluate by Full SQL Script Comparison\n> Directly compare the string content of each `Model Answer` with the `Ground Truth Answer` to see if they are exactly the same.\n\n| Model                   | Accuracy        |\n| ----------------------- | --------------- |\n| Gemma2-2b-it            | 3.33%           |\n| Fine-tuned Gemma2-2b-it | **33.33%**      |\n\n### Evaluate by SOTA LLM\n> Let the SOTA LLM act as a judge, providing `SQL Context`, `Query`, `Ground Truth Answer`, and `Model Answer`. Ask the SOTA LLM to determine whether each Model Answer can achieve the same effect as the Ground Truth Answer in answering the Query.\n\n| Model                   | Accuracy        |\n| ----------------------- | --------------- |\n| Gemma2-2b-it            | 23.33%          |\n| Fine-tuned Gemma2-2b-it | **60.00%**      |\n\n**References:**\n- [LLM-Finetuning GitHub Repository](https://github.com/ashishpatel26/LLM-Finetuning?tab=readme-ov-file)\n- [LLM Fine-tuning Chat Template](https://github.com/mst272/LLM-Dojo/tree/main/chat_template#gemma)\n- [Finetune Gemma-2b for Text to SQL](https://medium.com/@hayagriva99999/finetune-gemma-2b-for-text-to-sql-90041abdda70)\n- [Best Way To Fine-tune Your LLM Using a T4 GPU](https://jair-neto.medium.com/best-way-to-fine-tune-your-llm-using-a-t4-gpu-part-3-3-71c7d0514aa6)\n- [Conversation on Evaluation of Text-to-SQL Task](https://github.com/explodinggradients/ragas/issues/651)\n- [Steps By Step Tutorial To Fine Tune LLAMA 2 With Custom Dataset Using LoRA And QLoRA Techniques](https://www.youtube.com/watch?v=Vg3dS-NLUT4)\n- [Methods and tools for efficient training on a single GPU](https://huggingface.co/docs/transformers/perf_train_gpu_one#flash-attention-2)\n- [HuggingFace Developer Guides of Quantization](https://huggingface.co/docs/peft/developer_guides/quantization)\n- [What Rank r and Alpha To Use in LoRA in LLM ?](https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133)\n- [TaskType Parameter of LoRA Config](https://discuss.huggingface.co/t/task-type-parameter-of-loraconfig/52879/6)\n- [What Target Modules Should We Add When Training with LoRA](https://www.reddit.com/r/LocalLLaMA/comments/15sgg4m/what_modules_should_i_target_when_training_using/)\n- [Difference of DataCollator between CausalLM and Seq2Seq Model](https://gitea.exxedu.com/aibot/LLaMA-Factory/src/commit/3a666832c119606a8d5baf4694b96569bee18659/scripts/cal_ppl.py)","metadata":{}},{"cell_type":"markdown","source":"## **1. Preliminary Work**","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:23:48.587710Z","iopub.execute_input":"2024-10-02T10:23:48.588393Z","iopub.status.idle":"2024-10-02T10:23:49.823824Z","shell.execute_reply.started":"2024-10-02T10:23:48.588346Z","shell.execute_reply":"2024-10-02T10:23:49.822581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1-1. Install Packages**","metadata":{"id":"93lzZVsMazQN"}},{"cell_type":"code","source":"# install peft for parameter-efficient fine-tuning\n!pip install -q -U peft\n# install model, dataset, training booster, model evaluation, quantization from HuggingFace\n!pip install -q -U transformers datasets accelerate bitsandbytes trl\n\n# console log\nprint(\"1-1. pip install all packages and dependencies...done.\")","metadata":{"id":"9I8Q4Lh-aEIF","outputId":"e7a03c06-8a04-4215-bcaa-8afa0eacdbc9","execution":{"iopub.status.busy":"2024-10-02T10:23:49.826245Z","iopub.execute_input":"2024-10-02T10:23:49.826640Z","iopub.status.idle":"2024-10-02T10:24:23.290502Z","shell.execute_reply.started":"2024-10-02T10:23:49.826599Z","shell.execute_reply":"2024-10-02T10:24:23.289095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1-2. Import Packages**","metadata":{}},{"cell_type":"code","source":"### model training\nfrom trl import (\n    SFTConfig,\n    SFTTrainer\n) # use SFTConfig & SFTTrainer for easy, supervise fine-tuning\nfrom datasets import (\n    load_dataset, \n    concatenate_datasets\n) # load dataset from HuggingFace and concatenate train & test dataset for data visualization\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM,\n    TrainingArguments, \n    BitsAndBytesConfig,\n    DataCollatorForLanguageModeling,\n    TextStreamer\n) # load tokenizer, model, set training arguments, use quantization, create data collator and stream output text\nfrom peft import (\n    LoraConfig, \n    TaskType,\n    get_peft_model,\n    PeftModel\n) # PEFT\nfrom accelerate import PartialState\nimport numpy as np # numpy calculation\nimport torch # use torch.bfloat16\n\n### visualization\nimport matplotlib.pyplot as plt # create charts for data visualization\n\n### others\nfrom tqdm import tqdm # for displaying progess bar\nfrom typing import List # for clear typing\nimport pandas as pd # output excel\nimport random # for random sampling\nimport os # check if folder exists in directory or not\n\nprint(\"1-2. import all packages needed...done.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:24:23.292058Z","iopub.execute_input":"2024-10-02T10:24:23.292412Z","iopub.status.idle":"2024-10-02T10:24:41.644886Z","shell.execute_reply.started":"2024-10-02T10:24:23.292370Z","shell.execute_reply":"2024-10-02T10:24:41.643712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1-3. Hyperparameters Setting**\n\n**About Kaggle Secret**\nreference:\n- [Kaggle Feature Launch: User Secrets](https://www.kaggle.com/discussions/product-feedback/114053)","metadata":{}},{"cell_type":"code","source":"# when execute code on Kaggle\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\n# # when execute code on Localhost\n# import os\n# os.environ[\"HF_TOKEN\"] = \"...\"\n# HF_TOKEN = os.getenv(\"HF_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:24:41.647504Z","iopub.execute_input":"2024-10-02T10:24:41.648205Z","iopub.status.idle":"2024-10-02T10:24:41.794006Z","shell.execute_reply.started":"2024-10-02T10:24:41.648163Z","shell.execute_reply":"2024-10-02T10:24:41.792921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TL;DR**\n- To achieve performance with LoRA comparable to full finetuning, you should **train all Linear layers**. \n- Example : For LLaMA, these are: `gate_proj`, `down_proj`, `up_proj`, `q_proj`, `v_proj`, `k_proj`, and `o_proj`.","metadata":{}},{"cell_type":"code","source":"try:\n    DATASET = \"gretelai/synthetic_text_to_sql\"\n    MODEL_ID = \"google/gemma-2-2b-it\" \n    DATASET_DIRECTORY = \"/kaggle/working/\"\n    DEVICE = \"cuda\"\n    SEED = 2024\n    \n    # train eval split\n    TRAIN_SIZE = 5000\n    EVAL_SIZE = 2000\n    \n    # collect columns we don't need while training model\n    REMOVE_COLS = ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation']\n    \n    # hyperparameters of LoRA\n    LORA_RANK = 64 # if we want to teach some new to LLM, set this not smaller than 32 is better\n    LORA_ALPHA = 64 # set this the same as rank is better\n    LORA_DROPOUT = 0.05 # set as the article used\n    LORA_TARGET_MODULES = [\"gate_proj\", \"down_proj\", \"up_proj\", \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"] # set layers we want to add LoRA, here I set all linear layers without `lm_head`\n    \n    # hyperparameters of Training Arguments\n    OUTPUT_DIR = \"text2sql_finetuning\" # the directory saving our log & checkpoint model\n    OPTIMIZER = \"paged_adamw_8bit\"\n    LEARNING_RATE = 2e-4 # default\n    WARMUP_RATIO = 0.03\n    GRADIENT_ACCUMULUATION_STEPS = 2\n    LR_SCHEDULER_TYPE = \"cosine\" # set as the article used\n    LOGGING_STEPS = 100\n    SAVE_STRATEGY = \"epoch\"\n    PER_DEVICE_TRAIN_BATCH_SIZE = 1 # prevent out of memory\n    PER_DEVICE_EVAL_BATCH_SIZE = 1 # prevent out of memory\n    WEIGHT_DECAY = 0.05 # weight decay to apply to all layers except bias/LayerNorm weights\n    MAX_GRAD_NORM = 0.3 # maximum fradient normal (gradient clipping)\n    MAX_STEPS = 1000 # maximum number of training steps\n    \n    # finetuned model name (save model)\n    PEFT_MODEL_ID = \"lora_text2sql\"\n    \n    # finetuned model name (if we upload our model to Kaggle/HuggingFace, we should set the name of model)\n    VERSION = 1\n    NEW_MODEL = \"text2sql_gemma2_2b_it\" # when the model is in the directory `/kaggle/working/`, use this\n    KAGGLE_NEW_MODEL = \"None\" # when the model is uploaded to Kaggle as model, use this\n    \n    # LLM generation hyperparameters\n    MAX_NEW_TOKENS = 1000\n    TOP_P = 0.95\n    TOP_K = 50\n    TEMPERATURE = 0.7\n    \n    # how many test cases we want to evaluate\n    TEST_CASES_NUMBER = 30\n    \n    print(\"1-3. setting all parameters...done.\")\nexcept Exception as e:\n    print(\"1-3. Exception message as below:\\n\", e)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:24:41.795586Z","iopub.execute_input":"2024-10-02T10:24:41.796012Z","iopub.status.idle":"2024-10-02T10:24:41.807600Z","shell.execute_reply.started":"2024-10-02T10:24:41.795959Z","shell.execute_reply":"2024-10-02T10:24:41.806506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2. Load Dataset and Preprocess Data**","metadata":{}},{"cell_type":"markdown","source":"### **2-1. Load Dataset, Tokenizer**","metadata":{"id":"7_s4srVwa7oI"}},{"cell_type":"code","source":"# Load tokenizer of Gemma-2-2b-it with HuggingFace Token\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side=\"right\" # since pad_token = eos_token","metadata":{"id":"kI0hfJ31bjrO","execution":{"iopub.status.busy":"2024-10-02T10:24:41.809123Z","iopub.execute_input":"2024-10-02T10:24:41.809482Z","iopub.status.idle":"2024-10-02T10:24:43.498748Z","shell.execute_reply.started":"2024-10-02T10:24:41.809445Z","shell.execute_reply":"2024-10-02T10:24:43.497582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(DATASET)\ndataset = dataset.shuffle(seed=SEED)\n# for faster training, we only train with 5000 rows of train data for training and 2000 rows of train data for evaluation\ndataset[\"train\"] = dataset[\"train\"].train_test_split(train_size=TRAIN_SIZE, test_size=EVAL_SIZE, seed=SEED) \n\nprint(f\"Train dataset size: {len(dataset['train']['train'])} rows.\")\nprint(f\"Eval dataset size: {len(dataset['train']['test'])} rows.\")\nprint(f\"Test dataset size: {len(dataset['test'])} rows.\")\nprint(dataset)","metadata":{"id":"fXdqfxW0aElc","outputId":"6960417d-0800-42a4-de7b-1c7101f693fa","execution":{"iopub.status.busy":"2024-10-02T10:24:43.500113Z","iopub.execute_input":"2024-10-02T10:24:43.500468Z","iopub.status.idle":"2024-10-02T10:24:43.505485Z","shell.execute_reply.started":"2024-10-02T10:24:43.500426Z","shell.execute_reply":"2024-10-02T10:24:43.504377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2-2. Data Visualization**\n- the maximum tokenized `sql schema` & `sql code` length is **724** which is much smaller than the context length of `Gemma-2-2b-it` is **8192** tokens","metadata":{}},{"cell_type":"code","source":"# Input (sql prompt + sql context)\ntokenized_inputs = concatenate_datasets([dataset[\"train\"][\"train\"], dataset[\"train\"][\"test\"], dataset[\"test\"]]).\\\n    map(lambda x: tokenizer(x[\"sql_context\"]), batched=True)\ntokenized_input_lengths = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n\n# Output\ntokenized_outputs = concatenate_datasets([dataset[\"train\"][\"train\"], dataset[\"train\"][\"test\"], dataset[\"test\"]]).\\\n    map(lambda x: tokenizer(x[\"sql\"]), batched=True)\ntokenized_output_lengths = [len(x) for x in tokenized_outputs[\"input_ids\"]]\n\n# Console log\nprint(f\"maximum tokenized input (sql schema) length + maximum tokenized output (sql code) length = {max(tokenized_input_lengths) + max(tokenized_output_lengths)}\")\n\n# Histogram\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\naxes[0].hist(tokenized_input_lengths, bins=20, color=\"#19334d\")\naxes[0].set_xlabel(\"tokenized_sql_schema_length\")\naxes[0].set_ylabel(\"times\") \naxes[0].set_title(\"distribution of tokenized sql schema length\") \n\naxes[1].hist(tokenized_output_lengths, bins=20, color=\"#669900\")\naxes[1].set_xlabel(\"tokenized_sql_code_length\")\naxes[1].set_ylabel(\"times\")  \naxes[1].set_title(\"distribution of tokenized sql code length\") \n\nplt.tight_layout()\n\nplt.show()","metadata":{"id":"d7mRRHdIbv5O","outputId":"45dede71-ef2c-48e5-f78e-696d8729dff7","execution":{"iopub.status.busy":"2024-10-02T10:24:43.506969Z","iopub.execute_input":"2024-10-02T10:24:43.507387Z","iopub.status.idle":"2024-10-02T10:24:43.521149Z","shell.execute_reply.started":"2024-10-02T10:24:43.507335Z","shell.execute_reply":"2024-10-02T10:24:43.520133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2-3. Data Preprocess**\n\n- Use chat template to perform supervised fine-tuning on Gemma-2-2b-it\n\n**reference**\n- [LLM Fine-tuning Chat Template](https://github.com/mst272/LLM-Dojo/tree/main/chat_template#gemma)\n- [Finetune Gemma-2b for Text to SQL](https://medium.com/@hayagriva99999/finetune-gemma-2b-for-text-to-sql-90041abdda70)","metadata":{"id":"0llAPTDXb8mv"}},{"cell_type":"code","source":"# generate chat template\ndef generate_chat_template(sample):\n    system_prompt = \"You are a helpful assistant specialised on text-to-SQL. Given a question and context regarding one or more tables in database, your task is to generate a SQL query to answer questions.\"\n    if sample['sql_context']:\n        chat_template = f\"\"\"<start_of_turn>user {system_prompt}\nSQL Question: {sample['sql_prompt']}\nSQL Context: {sample['sql_context']}\nGenerated SQL Query: <end_of_turn>\n\n<start_of_turn>model {sample['sql']} <end_of_turn>\"\"\"\n    else:\n        chat_template = f\"\"\"<start_of_turn>user {system_prompt}\nSQL Question: {sample['sql_prompt']}\nGenerated SQL Query: <end_of_turn>\n\n<start_of_turn>model {sample['sql']} <end_of_turn>\"\"\"\n    return chat_template\n    \n# add prompt column to dataset\ntrain_prompt_column = [generate_chat_template(sample) for sample in dataset[\"train\"][\"train\"]]\neval_prompt_column = [generate_chat_template(sample) for sample in dataset[\"train\"][\"test\"]]\ndataset[\"train\"][\"train\"] = dataset[\"train\"][\"train\"].add_column(\"prompt\", train_prompt_column)\ndataset[\"train\"][\"test\"] = dataset[\"train\"][\"test\"].add_column(\"prompt\", eval_prompt_column)\n\n# preview\ndataset","metadata":{"id":"S5Tll2vXb2FV","outputId":"da198d7d-3346-45e3-abbc-3c73970fc89a","execution":{"iopub.status.busy":"2024-10-02T10:24:43.522402Z","iopub.execute_input":"2024-10-02T10:24:43.522764Z","iopub.status.idle":"2024-10-02T10:24:43.533270Z","shell.execute_reply.started":"2024-10-02T10:24:43.522728Z","shell.execute_reply":"2024-10-02T10:24:43.532297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenize prompt\ndef data_preprocess(sample):\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side=\"right\" # since pad_token = eos_token\n    return tokenizer(sample[\"prompt\"])\n\ntrain_tokenized_data = dataset[\"train\"][\"train\"].map(data_preprocess, batched=True, remove_columns=REMOVE_COLS)\neval_tokenized_data = dataset[\"train\"][\"test\"].map(data_preprocess, batched=True, remove_columns=REMOVE_COLS)\ntest_tokenized_data = dataset[\"test\"].map(data_preprocess, batched=True, remove_columns=REMOVE_COLS)\n\nprint(\"[TRAIN] tokenized_dataset:\\n\", train_tokenized_data)\nprint(\"[EVAL]  tokenized_dataset:\\n\", eval_tokenized_data)\nprint(\"[TEST]  tokenized_dataset:\\n\", test_tokenized_data)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:24:43.536765Z","iopub.execute_input":"2024-10-02T10:24:43.537074Z","iopub.status.idle":"2024-10-02T10:24:43.548164Z","shell.execute_reply.started":"2024-10-02T10:24:43.537040Z","shell.execute_reply":"2024-10-02T10:24:43.547158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2-4. Create DataCollator For CausalLM**","metadata":{}},{"cell_type":"code","source":"# Create DataCollator\n\"\"\"\nUse the end-of-sequence token as the padding token and set mlm=False. \nThis will use the inputs as labels shifted to the right by one element.\n\n# tokenizer.pad_token = tokenizer.eos_token:\nGenerally, pad_token is [PAD] by default and eos_token is [SEP] by default.\nTherefore, if we set pad_token as eos_token, it means that we set [SEP] as our pad_token instead of [PAD].\nThe reason we implement this is to let the model deal with the padding part more consistently and let us get better result.\n\"\"\"\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n# data_collator","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:24:43.549318Z","iopub.execute_input":"2024-10-02T10:24:43.549665Z","iopub.status.idle":"2024-10-02T10:24:43.559408Z","shell.execute_reply.started":"2024-10-02T10:24:43.549620Z","shell.execute_reply":"2024-10-02T10:24:43.558382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3. Fine-tune Gemma-2-2b-it**\n\n**Goal : prevent `CUDA out of memory` error**\n- Fine-tune method = **QLoRA (Quantization + LoRA)**","metadata":{}},{"cell_type":"markdown","source":"### **3-1. Load Model**\n-  **Gemma-2-2b-it**\n    - BitsAndBytesConfig setting reference: [Finetune Gemma-2b for Text to SQL](https://medium.com/@hayagriva99999/finetune-gemma-2b-for-text-to-sql-90041abdda70)","metadata":{}},{"cell_type":"code","source":"# load model with HuggingFace Token\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID, \n    device_map={\"\":0}, \n    quantization_config=quantization_config, \n    token=HF_TOKEN\n)\nmodel","metadata":{"id":"ABb1yRDQcBkv","execution":{"iopub.status.busy":"2024-10-02T10:24:43.561092Z","iopub.execute_input":"2024-10-02T10:24:43.561800Z","iopub.status.idle":"2024-10-02T10:24:43.575633Z","shell.execute_reply.started":"2024-10-02T10:24:43.561752Z","shell.execute_reply":"2024-10-02T10:24:43.574608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **3-2. Get PEFT Model with LoRA Adapter**\n\nif we set the LoRA config as follow:\n```json\n{\n    \"r\": 64,\n    \"lora_alpha\": 64,\n    \"target_modules\": [\"gate_proj\", \"down_proj\", \"up_proj\", \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    \"lora_dropout\": 0.05,\n    \"bias\": \"none\",\n    \"task_type\": TaskType.CAUSAL_LM\n}\n```\n\nthe trainable parameters are as follow:\n\n- **trainable params: 83,066,880 || all params: 2,697,408,768 || trainable%: 3.0795**","metadata":{}},{"cell_type":"code","source":"\"\"\"\nOverview of the supported task types:\n- SEQ_CLS: Text classification.\n- SEQ_2_SEQ_LM: Sequence-to-sequence language modeling.\n- CAUSAL_LM: Causal language modeling.\n- TOKEN_CLS: Token classification.\n- QUESTION_ANS: Question answering.\n- FEATURE_EXTRACTION: Feature extraction. Provides the hidden states which can be used as embeddings or features\n  for downstream tasks.\n\"\"\"\n\n# Define LoRA Config\nlora_config = LoraConfig(\n     r=LORA_RANK,\n     lora_alpha=LORA_ALPHA,\n     target_modules=LORA_TARGET_MODULES,\n     lora_dropout=LORA_DROPOUT,\n     bias=\"none\",\n     task_type=TaskType.CAUSAL_LM\n)\n\n# add LoRA adaptor\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"id":"YOz5D6jfcglr","execution":{"iopub.status.busy":"2024-10-02T10:24:43.577189Z","iopub.execute_input":"2024-10-02T10:24:43.577565Z","iopub.status.idle":"2024-10-02T10:24:43.586113Z","shell.execute_reply.started":"2024-10-02T10:24:43.577505Z","shell.execute_reply":"2024-10-02T10:24:43.585104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **3-3. Define Training Arguments**","metadata":{"id":"TWDjJ3hKc3_m"}},{"cell_type":"code","source":"# Define training arguments (SFTConfig)\ntraining_args = SFTConfig(\n    output_dir=OUTPUT_DIR,\n    optim=OPTIMIZER,\n    learning_rate=LEARNING_RATE,\n    warmup_ratio = WARMUP_RATIO,\n    gradient_accumulation_steps=GRADIENT_ACCUMULUATION_STEPS,\n    lr_scheduler_type=LR_SCHEDULER_TYPE,\n    logging_steps=LOGGING_STEPS,\n    save_strategy=SAVE_STRATEGY,\n    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n    weight_decay=WEIGHT_DECAY,\n    max_grad_norm=MAX_GRAD_NORM,\n    max_steps=MAX_STEPS,\n    report_to=\"none\",\n    dataset_text_field=\"prompt\",\n)\n\n# Create Trainer Instance\ntorch.cuda.empty_cache()\ntokenizer.padding_side = \"right\"\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    peft_config=lora_config,\n    data_collator=data_collator,\n    train_dataset=train_tokenized_data,\n    eval_dataset=eval_tokenized_data\n)\n\nmodel.config.use_cache = False  # not use cache while training","metadata":{"id":"dGhpERc8c2mp","execution":{"iopub.status.busy":"2024-10-02T10:24:43.589384Z","iopub.execute_input":"2024-10-02T10:24:43.590004Z","iopub.status.idle":"2024-10-02T10:24:43.598508Z","shell.execute_reply.started":"2024-10-02T10:24:43.589963Z","shell.execute_reply":"2024-10-02T10:24:43.597693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **3-4. Start Training**","metadata":{"id":"N-93Xyb4dV-r"}},{"cell_type":"code","source":"# train model\ntrainer.train()","metadata":{"id":"8iYaXCRkdbox","execution":{"iopub.status.busy":"2024-10-02T10:24:43.599908Z","iopub.execute_input":"2024-10-02T10:24:43.600572Z","iopub.status.idle":"2024-10-02T10:24:43.613862Z","shell.execute_reply.started":"2024-10-02T10:24:43.600516Z","shell.execute_reply":"2024-10-02T10:24:43.612880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **3-5. Merge and Save Finetuned Model & Tokenizer to Disk**\n- Merge the base model with LoRA weights\n    - code reference: [Finetune Gemma-2b for Text to SQL](https://medium.com/@hayagriva99999/finetune-gemma-2b-for-text-to-sql-90041abdda70)","metadata":{"id":"RERkk6cCelb1"}},{"cell_type":"code","source":"# Save our LoRA model & tokenizer results\ntrainer.model.save_pretrained(PEFT_MODEL_ID)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the model with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"cpu\", # since merge_and_unload() might perform weird when device_map={\"\":0}, we turned into use device_map=\"cpu\"\n    token=HF_TOKEN\n)\nfinetuned_model = PeftModel.from_pretrained(base_model, \"/kaggle/input/lora-text2sql\")\nfinetuned_model = finetuned_model.merge_and_unload()\n\n# Save the merged model\nfinetuned_model.save_pretrained(NEW_MODEL, safe_serialization=True)\ntokenizer.save_pretrained(NEW_MODEL)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"id":"ZrKOWJ40d6RQ","execution":{"iopub.status.busy":"2024-10-02T10:24:43.615258Z","iopub.execute_input":"2024-10-02T10:24:43.615958Z","iopub.status.idle":"2024-10-02T10:26:05.361872Z","shell.execute_reply.started":"2024-10-02T10:24:43.615909Z","shell.execute_reply":"2024-10-02T10:26:05.360528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete models we don't need to reduce memory usage\nimport gc\ndel base_model\ndel finetuned_model\ndel tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:26:05.363769Z","iopub.execute_input":"2024-10-02T10:26:05.364355Z","iopub.status.idle":"2024-10-02T10:26:06.170100Z","shell.execute_reply.started":"2024-10-02T10:26:05.364292Z","shell.execute_reply":"2024-10-02T10:26:06.168642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4. Evaluate the performance of LoRA fine-tuned CodeGemma-1.1-2b**","metadata":{}},{"cell_type":"markdown","source":"### **4-1. Load base model, tokenizer, fine-tuned model, and Test Dataset**","metadata":{}},{"cell_type":"code","source":"# load two tokenizers (since they are on different devices)\nbase_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\nbase_tokenizer.pad_token = base_tokenizer.eos_token\nbase_tokenizer.padding_side=\"right\" # since pad_token = eos_token\ntext2sql_tokenizer = AutoTokenizer.from_pretrained(NEW_MODEL, token=HF_TOKEN)\n\n# load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map={\"\":0}, \n    token=HF_TOKEN\n)\n\n# load fine-tuned model\ntext2sql_model = AutoModelForCausalLM.from_pretrained(NEW_MODEL, token=HF_TOKEN)\ntext2sql_model = text2sql_model\n\n# load test dataset from the hub\ndataset = load_dataset(DATASET)\ntest_dataset = dataset[\"test\"]\ntest_ids = random.sample(range(0, test_dataset.num_rows), TEST_CASES_NUMBER)\n\n# print status quo\nprint(\"base tokenizer, finetuned tokenizer, base model, finetuned model and test dataset are all loaded...done.\")\nprint(\"test dataset :\")\nprint(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:26:35.466572Z","iopub.execute_input":"2024-10-02T10:26:35.467060Z","iopub.status.idle":"2024-10-02T10:27:03.352525Z","shell.execute_reply.started":"2024-10-02T10:26:35.467017Z","shell.execute_reply":"2024-10-02T10:27:03.351502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4-2. Define Evaluate Function**","metadata":{}},{"cell_type":"code","source":"# define function for evaluate the performance of transforming text to sql\ndef evaluate(mode: str, sample: dict) -> (List[int], List[str]):\n    # mode can only be \"base\" or \"text2sql\"\n    assert mode in [\"base\", \"text2sql\"]\n\n    # generate chat template\n    system_prompt = \"You are a helpful assistant specialised on text-to-SQL. Given a question and context regarding one or more tables in database, your task is to generate a SQL query to answer questions.\"\n    if sample['sql_context']:\n        chat_template = f\"\"\"<start_of_turn>user {system_prompt}\nSQL Question: {sample['sql_prompt']}\nSQL Context: {sample['sql_context']}\nGenerated SQL Query: <end_of_turn>\n\n<start_of_turn>model \n\n\"\"\"\n    else:\n        chat_template = f\"\"\"<start_of_turn>user {system_prompt}\nSQL Question: {sample['sql_prompt']}\nGenerated SQL Query: <end_of_turn>\n\n<start_of_turn>model \n\n\"\"\"\n\n    if mode == \"base\":\n         # tokenize chat template\n        model_inputs = base_tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n        input_length = model_inputs.input_ids.shape[1]\n        # generate results with base model\n        base_output_ids = base_model.generate(input_ids=model_inputs.input_ids, \n                                              max_new_tokens=MAX_NEW_TOKENS, \n                                              do_sample=True, \n                                              top_p=TOP_P,\n                                              top_k=TOP_K,\n                                              temperature=TEMPERATURE\n                                             )\n        output = base_tokenizer.decode(base_output_ids[0][input_length:].detach().cpu().numpy(), skip_special_tokens=True)\n\n    if mode == \"text2sql\":\n        # tokenize chat template\n        model_inputs = text2sql_tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=True).to(\"cpu\")\n        input_length = model_inputs.input_ids.shape[1]\n        # generate results with finetuned text2sql model\n        text2sql_output_ids = text2sql_model.generate(input_ids=model_inputs.input_ids, \n                                              max_new_tokens=MAX_NEW_TOKENS, \n                                              do_sample=True, \n                                              top_p=TOP_P,\n                                              top_k=TOP_K,\n                                              temperature=TEMPERATURE\n                                             )\n        output = text2sql_tokenizer.decode(text2sql_output_ids[0][input_length:].detach().cpu().numpy(), skip_special_tokens=True)\n\n    # post process\n    output = output.replace(\"\\n\", \"\").replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n        \n    # ground truth\n    ground_truth = sample[\"sql\"].strip()\n    \n    # print the result\n    print(f\"\"\"Question:\n>>> {sample['sql_prompt']}\nGround Truth Answer:\n>>> {ground_truth}\n{mode.capitalize()} Model Answer:\n>>> {output}\"\"\")\n    print()\n    \n    # output schema, query, ground truth answer, base model/text2sql model answer\n    row_answer = [sample['sql_context'], sample['sql_prompt'], ground_truth, output]\n    \n    # check and calculate the accuracy\n    print(\"Result:\")\n    if output == ground_truth:\n        print(f\"- {mode.capitalize()} Model Success.\")\n        result = 1\n    else:\n        print(f\"- {mode.capitalize()} Model Failed.\")\n        result = 0\n    return result, row_answer\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:27:03.354874Z","iopub.execute_input":"2024-10-02T10:27:03.355323Z","iopub.status.idle":"2024-10-02T10:27:03.369502Z","shell.execute_reply.started":"2024-10-02T10:27:03.355273Z","shell.execute_reply":"2024-10-02T10:27:03.368366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4-3. (Optional) Test if our fine-tuned text-to-sql model works**","metadata":{"id":"0XGJ5QG2frXZ"}},{"cell_type":"code","source":"# get one sample\nmode = \"text2sql\"\nsample = test_dataset[0]\nresult, row_answer = evaluate(mode, sample)\nprint(\"-\"*30)\nprint(f\"[{mode.capitalize()} Model] result & row_answer are as follows:\")\nprint(result)\nprint(row_answer)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:26:06.190202Z","iopub.status.idle":"2024-10-02T10:26:06.190763Z","shell.execute_reply.started":"2024-10-02T10:26:06.190480Z","shell.execute_reply":"2024-10-02T10:26:06.190508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4-4. Evaluate the performance of the base model**\n- `Gemma-2-2b-it`","metadata":{}},{"cell_type":"code","source":"# control reproducibility\nrandom.seed(SEED)\n\n# evaluate and calculate accuracy(%)\nbase_model_logs = []\nbase_success_or_failed = []\nfor idx in tqdm(test_ids):\n    sample = test_dataset[idx]\n    result, row_answer = evaluate(\"base\", sample)\n    base_model_logs.append(row_answer)\n    base_success_or_failed.append(result)\n    \n# compute accuracy\nbase_accuracy = sum(base_success_or_failed)/len(base_success_or_failed)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:27:19.992195Z","iopub.execute_input":"2024-10-02T10:27:19.992636Z","iopub.status.idle":"2024-10-02T10:27:24.284196Z","shell.execute_reply.started":"2024-10-02T10:27:19.992593Z","shell.execute_reply":"2024-10-02T10:27:24.283245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4-5. Evaluate the performance of our finetuned model**\n- `LoRA Fine-tuned Text-to-SQL Gemma-2-2b-it`","metadata":{}},{"cell_type":"code","source":"# control reproducibility\nrandom.seed(SEED)\n\n# evaluate and calculate accuracy(%)\ntext2sql_model_logs = []\ntext2sql_success_or_failed = []\nfor idx in tqdm(test_ids):\n    sample = test_dataset[idx]\n    result, row_answer = evaluate(\"text2sql\", sample)\n    text2sql_model_logs.append(row_answer)\n    text2sql_success_or_failed.append(result)\n\n# compute accuracy\ntext2sql_accuracy = sum(text2sql_success_or_failed)/len(text2sql_success_or_failed)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:28:08.862940Z","iopub.execute_input":"2024-10-02T10:28:08.864126Z","iopub.status.idle":"2024-10-02T10:28:37.265591Z","shell.execute_reply.started":"2024-10-02T10:28:08.864075Z","shell.execute_reply":"2024-10-02T10:28:37.263667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **4-6. Print Test Result & Save As Excel File for SOTA LLM Judging**","metadata":{}},{"cell_type":"code","source":"print(f\"\"\"\n\nACCURACY RESULT BY CHECKING IF THE SQL CODE IS THE SAME AS THE GROUND TRUTH SQL CODE\n---------------------------------------------------\n| [Base Model]     Text-to-SQL Accuracy: {base_accuracy*100:.2f}% |\n| [Text2sql Model] Text-to-SQL Accuracy: {text2sql_accuracy*100:.2f}% |\n---------------------------------------------------\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:26:06.203070Z","iopub.status.idle":"2024-10-02T10:26:06.203567Z","shell.execute_reply.started":"2024-10-02T10:26:06.203298Z","shell.execute_reply":"2024-10-02T10:26:06.203318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base df\nbase_df = pd.DataFrame(base_model_logs, columns=[\n    \"Table Schema\", \"Question\", \"Ground Truth SQL Code\", \"Base Model SQL Code\"\n])\n\n# text2sql df\ntext2sql_df = pd.DataFrame(text2sql_model_logs, columns=[\n    \"Table Schema\", \"Question\", \"Ground Truth SQL Code\", \"Text2sql Model SQL Code\"\n])\n\n# filename\nbase_fname = f\"text2sql_test_result_of_base_model_v{VERSION}.xlsx\"\ntext2sql_fname = f\"text2sql_test_result_of_text2sql_model_v{VERSION}.xlsx\"\n\n# output excel\nbase_df.to_excel(base_fname, index=False)\ntext2sql_df.to_excel(text2sql_fname, index=False)\nprint(f\"test results are already saved.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:26:06.205016Z","iopub.status.idle":"2024-10-02T10:26:06.205450Z","shell.execute_reply.started":"2024-10-02T10:26:06.205232Z","shell.execute_reply":"2024-10-02T10:26:06.205252Z"},"trusted":true},"execution_count":null,"outputs":[]}]}